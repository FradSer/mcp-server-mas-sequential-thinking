"""Model-specific optimization for different LLM providers."""

import re

from mcp_server_mas_sequential_thinking.infrastructure.logging_config import get_logger

logger = get_logger(__name__)


class ModelCharacteristics:
    """Characteristics and tendencies of different LLM models."""

    DEEPSEEK_V3 = {
        "academic_bias": 0.9,  # Very high tendency toward academic language
        "technical_solution_bias": 0.8,  # Converts everything to technical problems
        "structure_obsession": 0.85,  # Over-structures responses
        "concept_density": 0.9,  # Packs too many concepts
        "human_relatability": 0.3,  # Low natural human connection
        "practical_focus": 0.2,  # Poor at practical advice
    }

    CLAUDE_3_5 = {
        "academic_bias": 0.4,
        "technical_solution_bias": 0.3,
        "structure_obsession": 0.5,
        "concept_density": 0.4,
        "human_relatability": 0.8,
        "practical_focus": 0.7,
    }

    GPT_4 = {
        "academic_bias": 0.5,
        "technical_solution_bias": 0.4,
        "structure_obsession": 0.6,
        "concept_density": 0.5,
        "human_relatability": 0.7,
        "practical_focus": 0.6,
    }


class DeepSeekV3Optimizer:
    """Specific optimizer for DeepSeek V3's tendencies."""

    def __init__(self) -> None:
        self.academic_indicators = [
            "æ¡†æ¶", "æ¨¡å‹", "ç»´åº¦", "çŸ©é˜µ", "ç®—æ³•", "ç³»ç»Ÿ", "æœºåˆ¶",
            "æ¶æ„", "å¼•æ“", "åè®®", "æ–¹ç¨‹", "å…¬å¼", "å˜é‡", "å‚æ•°",
            "é˜ˆå€¼", "æŒ‡æ ‡", "è¯„ä¼°", "éªŒè¯", "å®æ–½", "éƒ¨ç½²"
        ]

        self.tech_solution_indicators = [
            "èŠ¯ç‰‡", "AI", "åŒºå—é“¾", "é‡å­", "åŸºå› ç¼–è¾‘", "è„‘æœºæ¥å£",
            "äº‘è®¡ç®—", "å¤§æ•°æ®", "ç¥ç»ç½‘ç»œ", "æ·±åº¦å­¦ä¹ ", "ç®—åŠ›",
            "æ•°å­—åŒ–", "æ™ºèƒ½åŒ–", "è‡ªåŠ¨åŒ–", "å¹³å°", "ç³»ç»Ÿ"
        ]

        self.over_structure_patterns = [
            r"#{3,}",  # Too many heading levels
            r"\|\s*.*\s*\|",  # Tables
            r"```[\s\S]*?```",  # Code blocks for non-code content
            r"^\d+\.\s+.*â†’.*â†’",  # Complex numbered flows
        ]

    def optimize_prompt_for_deepseek(self, original_prompt: str, question_type: str = "philosophical") -> str:
        """Optimize prompt specifically for DeepSeek V3 to get better responses."""
        # Base optimization for all question types
        optimized_prompt = f"""è¯·ç”¨ç®€å•ã€äººæ€§åŒ–çš„è¯­è¨€å›ç­”ï¼Œé¿å…è¿‡åº¦ç†è®ºåŒ–ã€‚

é‡è¦æŒ‡å¯¼åŸåˆ™ï¼š
- ç”¨æ—¥å¸¸è¯­è¨€ï¼Œä¸è¦å­¦æœ¯æœ¯è¯­
- ç»™å‡ºå®ç”¨çš„æ€è€ƒï¼Œè€Œä¸æ˜¯æŠ½è±¡æ¡†æ¶
- ä¿æŒæ¸©åº¦æ„Ÿï¼Œåƒå’Œæœ‹å‹å¯¹è¯
- ä¸è¦è®¾è®¡æŠ€æœ¯æ–¹æ¡ˆæˆ–ç³»ç»Ÿ
- é¿å…è¡¨æ ¼ã€å…¬å¼ã€å¤æ‚ç»“æ„

{original_prompt}

è¯·ç”¨å¹³å®çš„è¯­è¨€ï¼Œä»äººçš„è§’åº¦æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"""

        # Question-type specific optimizations
        if question_type == "philosophical":
            optimized_prompt += """

ç‰¹åˆ«è¦æ±‚ï¼šè¿™æ˜¯ä¸€ä¸ªå…³äºäººç”Ÿçš„é—®é¢˜ï¼Œè¯·ï¼š
- æ‰¿è®¤é—®é¢˜çš„å¤æ‚æ€§ï¼Œä½†ç»™å‡ºå¯ç†è§£çš„æ€è€ƒ
- åˆ†äº«ä¸åŒäººå¯èƒ½çš„æ„Ÿå—å’Œæƒ³æ³•
- æä¾›å®é™…çš„ç”Ÿæ´»è§†è§’ï¼Œè€Œä¸æ˜¯ç†è®ºåˆ†æ
- è®©å›ç­”èƒ½å¤Ÿå¸®åŠ©æé—®è€…æ€è€ƒï¼Œè€Œä¸æ˜¯æ˜¾ç¤ºçŸ¥è¯†"""

        elif question_type == "creative":
            optimized_prompt += """

ç‰¹åˆ«è¦æ±‚ï¼šè¿™æ˜¯ä¸€ä¸ªåˆ›æ„é—®é¢˜ï¼Œè¯·ï¼š
- æä¾›å…·ä½“å¯è¡Œçš„æƒ³æ³•
- ç”¨ä¾‹å­è¯´æ˜ï¼Œè€Œä¸æ˜¯æŠ½è±¡æ¦‚å¿µ
- ä¿æŒåˆ›æ„çš„è¶£å‘³æ€§å’Œå¯æ“ä½œæ€§
- é¿å…è®¾è®¡å¤æ‚çš„ç³»ç»Ÿæˆ–æµç¨‹"""

        elif question_type == "factual":
            optimized_prompt += """

ç‰¹åˆ«è¦æ±‚ï¼šè¿™æ˜¯ä¸€ä¸ªäº‹å®é—®é¢˜ï¼Œè¯·ï¼š
- ç›´æ¥å›ç­”æ ¸å¿ƒä¿¡æ¯
- ç”¨ç®€å•çš„è¯­è¨€è§£é‡Š
- å¦‚æœå¤æ‚ï¼Œç”¨æ¯”å–»æˆ–ä¾‹å­å¸®åŠ©ç†è§£
- é¿å…ä¸å¿…è¦çš„èƒŒæ™¯å±•å¼€"""

        return optimized_prompt

    def post_process_deepseek_response(self, response: str) -> str:
        """Post-process DeepSeek V3 response to remove problematic patterns."""
        # Remove excessive academic language
        response = self._simplify_academic_language(response)

        # Remove technical solutions for non-technical problems
        response = self._remove_tech_solutions(response)

        # Simplify over-structured content
        response = self._simplify_structure(response)

        # Add human warmth if missing
        return self._add_human_connection(response)


    def _simplify_academic_language(self, text: str) -> str:
        """Replace academic jargon with simpler language."""
        replacements = {
            "ç»¼åˆå†³ç­–æ¡†æ¶": "æ€è€ƒæ–¹å¼",
            "å¤šç»´åº¦åˆ†æ": "ä»ä¸åŒè§’åº¦çœ‹",
            "ç³»ç»Ÿæ€§æ€ç»´": "å…¨é¢æ€è€ƒ",
            "ä¼˜åŒ–ç®—æ³•": "æ›´å¥½çš„æ–¹æ³•",
            "å®æ–½è·¯å¾„": "å…·ä½“åšæ³•",
            "è¯„ä¼°æœºåˆ¶": "å¦‚ä½•åˆ¤æ–­",
            "ç›‘æµ‹æŒ‡æ ‡": "è§‚å¯Ÿè¦ç‚¹",
            "åè°ƒæœºåˆ¶": "é…åˆæ–¹å¼",
            "åé¦ˆå¾ªç¯": "ç›¸äº’å½±å“",
            "è¿­ä»£ä¼˜åŒ–": "ä¸æ–­æ”¹è¿›"
        }

        for academic, simple in replacements.items():
            text = text.replace(academic, simple)

        return text

    def _remove_tech_solutions(self, text: str) -> str:
        """Remove inappropriate technical solutions."""
        # Remove sentences containing tech buzzwords
        lines = text.split("\n")
        filtered_lines = []

        for line in lines:
            has_tech_overkill = any(
                indicator in line for indicator in [
                    "åŒºå—é“¾", "é‡å­", "åŸºå› ç¼–è¾‘", "è„‘æœºæ¥å£", "AIèŠ¯ç‰‡",
                    "ç¥ç»ç½‘ç»œè®­ç»ƒ", "ç®—æ³•ä¼˜åŒ–", "æ•°æ®æŒ–æ˜", "æœºå™¨å­¦ä¹ "
                ]
            )

            if not has_tech_overkill or "æŠ€æœ¯" in line:  # Keep if explicitly about technology
                filtered_lines.append(line)

        return "\n".join(filtered_lines)

    def _simplify_structure(self, text: str) -> str:
        """Simplify overly complex structure."""
        # Remove excessive headers (keep max 2 levels)
        text = re.sub(r"#{4,}", "###", text)

        # Remove complex tables for simple content
        lines = text.split("\n")
        filtered_lines = []
        in_table = False

        for line in lines:
            if "|" in line and "-" in line:  # Table separator
                in_table = True
                continue
            if "|" in line and in_table:  # Table content
                continue
            in_table = False
            filtered_lines.append(line)

        # Remove code blocks that aren't actually code
        text = "\n".join(filtered_lines)
        return re.sub(r"```[^`]*?```", "", text, flags=re.DOTALL)


    def _add_human_connection(self, text: str) -> str:
        """Add human warmth if the response is too cold."""
        # Check if response lacks human elements
        human_indicators = ["æ„Ÿå—", "ä½“éªŒ", "æƒ³æ³•", "è®¤ä¸º", "è§‰å¾—", "å¯èƒ½", "æˆ–è®¸"]

        if not any(indicator in text for indicator in human_indicators):
            # Add a more human opening
            if text.startswith("#"):
                text = re.sub(r"^#[^#]*\n", "", text)

            text = f"è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„é—®é¢˜ã€‚{text}"

        # Soften overly definitive statements
        text = re.sub(r"å¿…é¡»", "å¯ä»¥è€ƒè™‘", text)
        text = re.sub(r"åº”è¯¥å»ºç«‹", "ä¹Ÿè®¸å¯ä»¥", text)
        text = re.sub(r"éœ€è¦å®æ–½", "å¯ä»¥å°è¯•", text)

        return text

    def assess_response_quality(self, response: str, question: str) -> dict[str, float]:
        """Assess how well the response matches the question."""
        scores = {}

        # Academic overload score (lower is better)
        academic_count = sum(1 for indicator in self.academic_indicators if indicator in response)
        scores["academic_overload"] = min(academic_count / 10, 1.0)

        # Tech solution inappropriateness (lower is better)
        tech_count = sum(1 for indicator in self.tech_solution_indicators if indicator in response)
        is_tech_question = any(word in question for word in ["æŠ€æœ¯", "ç§‘æŠ€", "AI", "è®¡ç®—æœº"])
        scores["tech_inappropriateness"] = 0 if is_tech_question else min(tech_count / 5, 1.0)

        # Structure complexity (lower is better)
        structure_score = 0
        for pattern in self.over_structure_patterns:
            matches = len(re.findall(pattern, response, re.MULTILINE))
            structure_score += matches
        scores["structure_complexity"] = min(structure_score / 5, 1.0)

        # Human relatability (higher is better)
        human_words = ["æ„Ÿå—", "ä½“éªŒ", "æƒ³æ³•", "è®¤ä¸º", "è§‰å¾—", "å¯èƒ½", "æˆ–è®¸", "æœ‰æ—¶", "é€šå¸¸"]
        human_count = sum(1 for word in human_words if word in response)
        scores["human_relatability"] = min(human_count / 5, 1.0)

        # Overall quality (higher is better)
        scores["overall_quality"] = (
            (1 - scores["academic_overload"]) * 0.3 +
            (1 - scores["tech_inappropriateness"]) * 0.2 +
            (1 - scores["structure_complexity"]) * 0.2 +
            scores["human_relatability"] * 0.3
        )

        return scores


class ModelOptimizer:
    """General model optimizer that selects appropriate strategies."""

    def __init__(self, model_name: str) -> None:
        self.model_name = model_name.lower()
        self.deepseek_optimizer = DeepSeekV3Optimizer()

    def optimize_for_model(self, prompt: str, question_type: str = "general") -> str:
        """Optimize prompt based on model characteristics."""
        if "deepseek" in self.model_name:
            return self.deepseek_optimizer.optimize_prompt_for_deepseek(prompt, question_type)
        if "claude" in self.model_name:
            # Claude generally produces good responses, minimal optimization needed
            return f"{prompt}\n\nè¯·ç”¨è‡ªç„¶ã€æ˜“æ‡‚çš„è¯­è¨€å›ç­”ï¼Œä¿æŒäººæ€§åŒ–çš„è¡¨è¾¾ã€‚"
        if "gpt" in self.model_name:
            # GPT tends to be verbose, ask for conciseness
            return f"{prompt}\n\nè¯·ç®€æ´æ˜äº†åœ°å›ç­”ï¼Œé¿å…è¿‡åº¦å±•å¼€ã€‚"
        # Generic optimization
        return f"{prompt}\n\nè¯·ç”¨ç®€å•ã€å®ç”¨çš„è¯­è¨€å›ç­”ã€‚"

    def post_process_response(self, response: str) -> str:
        """Post-process response based on model characteristics."""
        if "deepseek" in self.model_name:
            return self.deepseek_optimizer.post_process_deepseek_response(response)
        # Basic post-processing for other models
        return response

    def get_quality_assessment(self, response: str, question: str) -> dict[str, float]:
        """Get quality assessment for the response."""
        if "deepseek" in self.model_name:
            return self.deepseek_optimizer.assess_response_quality(response, question)
        # Basic assessment for other models
        return {"overall_quality": 0.7}  # Assume decent quality


# Factory function
def create_model_optimizer(model_name: str) -> ModelOptimizer:
    """Create appropriate model optimizer."""
    return ModelOptimizer(model_name)


# Quick test function
def test_deepseek_optimization() -> None:
    """Test the DeepSeek optimization."""
    optimizer = DeepSeekV3Optimizer()

    # Test philosophical question optimization
    original_prompt = "å¦‚æœç”Ÿå‘½ç»ˆå°†ç»“æŸï¼Œæˆ‘ä»¬ä¸ºä»€ä¹ˆè¦æ´»ç€ï¼Ÿ"
    optimizer.optimize_prompt_for_deepseek(original_prompt, "philosophical")


    # Test response post-processing
    problematic_response = """
# ğŸŒ ç”Ÿå‘½æ„ä¹‰ç»¼åˆæ±‚è§£æ¡†æ¶

## ğŸ§¬ ç”Ÿç‰©ç»´åº¦é”šå®šç‚¹
**è¿›åŒ–ç°å®æ€§å¹³è¡¡**
- é‡‡ç”¨åŸºå› ç¼–è¾‘åº”åŒæ­¥å»ºç«‹å…¨çƒä¼¦ç†ç›‘å¯Ÿè”ç›Ÿ
- å°†æä»æ ¸æ¿€æ´»èƒ½é‡å¯¼å‘è‰ºæœ¯åˆ›é€ ä¸ç§‘å­¦çªç ´

## ğŸ§  è®¤çŸ¥é‡å»ºæ¨¡å‹
| å®¢è§‚æœ‰é™æ€§è®¤çŸ¥ | ä¸»è§‚æ°¸æ’å»ºæ„ |
|--------------|--------------|
| ç«¯ç²’æŸè€—é€Ÿåº¦ç›‘æµ‹ | è„‘æœºæ¥å£è®°å¿†æ™¶æ ¼å­˜å‚¨ |

## ğŸš€ æ‰§è¡Œè·¯çº¿å›¾
**ä¸‰æœŸé˜¶æ¢¯æˆ˜ç•¥**
- å»ºç«‹å…¨çƒç”Ÿå‘½æ¡£æ¡ˆé“¾ï¼ˆGLACï¼‰
- é‡å­ç”Ÿç‰©èŠ¯ç‰‡ä¸´åºŠåº”ç”¨
"""

    optimizer.post_process_deepseek_response(problematic_response)


    # Quality assessment
    quality = optimizer.assess_response_quality(problematic_response, original_prompt)
    for _metric, _score in quality.items():
        pass


if __name__ == "__main__":
    test_deepseek_optimization()
