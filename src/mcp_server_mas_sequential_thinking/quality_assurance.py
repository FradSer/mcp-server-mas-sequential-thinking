"""Quality assurance and evaluation system using Agno Evals."""

import logging
from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Optional, Any

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat

from .models import ThoughtData
from .intelligent_coordinator import CoordinationPlan
from .route_execution import ExecutionMode
from .modernized_config import get_model_config

logger = logging.getLogger(__name__)


@dataclass
class SystemPerformanceMetrics:
    """Comprehensive system performance metrics."""

    coordination_accuracy: float
    execution_consistency: float
    response_quality: float
    efficiency_score: float
    processing_time: float
    cost_effectiveness: float

    overall_score: float
    improvement_suggestions: List[str]


class CoordinationAccuracyEvaluator:
    """Evaluates coordination decision quality using Agno AccuracyEval."""

    def __init__(self):
        # Use lightweight model for evaluation
        config = get_model_config()
        self.eval_model = config.create_agent_model()

    def create_coordination_evaluation(
        self,
        thought: ThoughtData,
        plan: CoordinationPlan,
        response: str
    ) -> AccuracyEval:
        """Create coordination accuracy evaluation."""

        # Detect if content is primarily Chinese to use appropriate evaluation approach
        chinese_chars = len([c for c in thought.thought if '\u4e00' <= c <= '\u9fff'])
        is_chinese_content = chinese_chars > len(thought.thought) * 0.3

        if is_chinese_content:
            # Use Chinese-specific evaluation approach
            input_text = f"""
ÂàÜÊûê‰ª•‰∏ã‰∏≠ÊñáÊÄùÁª¥Â§ÑÁêÜÁöÑÂçèË∞ÉÂÜ≥Á≠ñÊòØÂê¶ÂêàÁêÜÔºö

ÂéüÂßãÊÄùÁª¥: "{thought.thought}"
ÊÄùÁª¥‰∏ä‰∏ãÊñá: Á¨¨{thought.thought_number}‰∏™ÊÄùÁª¥ÔºåÂÖ±{thought.total_thoughts}‰∏™Ôºå‰∏ã‰∏ÄÊ≠•ÈúÄË¶ÅÔºö{thought.next_needed}

ÂçèË∞ÉÂÜ≥Á≠ñ:
- Á≠ñÁï•: {plan.strategy.value}
- Â§çÊùÇÂ∫¶: {plan.complexity_level.value} (ËØÑÂàÜ: {plan.complexity_score:.1f}/100)
- ÊâßË°åÊ®°Âºè: {plan.execution_mode.value}
- ‰∏ìÂÆ∂ÁªÑÂêà: {plan.specialist_roles}
- ÂçèË∞ÉÊñπÂºè: {plan.coordination_strategy}
- ÁΩÆ‰ø°Â∫¶: {plan.confidence:.2f}

ÂÆûÈôÖÂìçÂ∫î: "{response[:200]}..."

ËØ∑ËØÑ‰º∞Ëøô‰∏™ÂçèË∞ÉÂÜ≥Á≠ñÁöÑÂáÜÁ°ÆÊÄßÂíåÂêàÁêÜÊÄßÔºåÁâπÂà´ËÄÉËôë‰∏≠ÊñáÂì≤Â≠¶ÊÄùÁª¥ÁöÑÁâπÁÇπ„ÄÇ
"""

            expected_output = f"""
Âü∫‰∫é‰∏≠ÊñáÊÄùÁª¥ÁöÑÂ§çÊùÇÂ∫¶ÂíåÊñáÂåñÂÜÖÊ∂µÔºåÂçèË∞ÉÂÜ≥Á≠ñËØÑ‰º∞Ôºö

1. Â§çÊùÇÂ∫¶ËØÑ‰º∞{'' if plan.complexity_score >= 30 else 'ÂèØËÉΩ'}ÂáÜÁ°Æ
2. Á≠ñÁï•ÈÄâÊã©{'' if plan.strategy.value != 'single_agent' or plan.complexity_score < 30 else 'ÂèØËÉΩ'}ÂêàÁêÜ
3. ‰∏ìÂÆ∂ÁªÑÂêà{'' if len(plan.specialist_roles) >= 2 or plan.complexity_score < 30 else 'ÂèØËÉΩ'}ÈÄÇÂΩì
4. ÊâßË°åÊ®°Âºè‰∏éÁ≠ñÁï•‰∏ÄËá¥
5. ÊñáÂåñÈÄÇÂ∫îÊÄßËâØÂ•Ω

ÊÄª‰ΩìËØÑ‰º∞ÔºöÂçèË∞ÉÂÜ≥Á≠ñ{'ÂêàÁêÜ' if plan.confidence > 0.5 else 'ÈúÄË¶ÅÊîπËøõ'}
"""

            agent_instructions = [
                "‰Ω†ÊòØ‰∏≠ÊñáÊÄùÁª¥Â§ÑÁêÜÂçèË∞ÉÂÜ≥Á≠ñÁöÑË¥®ÈáèËØÑ‰º∞‰∏ìÂÆ∂„ÄÇ",
                "ËØÑ‰º∞ÂçèË∞ÉÂÜ≥Á≠ñÊòØÂê¶ÂêàÁêÜÔºåÂåÖÊã¨Â§çÊùÇÂ∫¶ËØÑ‰º∞„ÄÅÁ≠ñÁï•ÈÄâÊã©„ÄÅ‰∏ìÂÆ∂ÁªÑÂêàÁ≠â„ÄÇ",
                "ÁâπÂà´ÂÖ≥Ê≥®‰∏≠ÊñáÊÄùÁª¥ÁöÑËØ≠‰πâÂ§çÊùÇÂ∫¶„ÄÅÂì≤Â≠¶Ê∑±Â∫¶ÂíåÊñáÂåñÂÜÖÊ∂µ„ÄÇ",
                "Âü∫‰∫éÊÄùÁª¥ÂÜÖÂÆπÁöÑÂÆûÈôÖÂ§çÊùÇÂ∫¶ÔºåÂà§Êñ≠ÂçèË∞ÉÂÜ≥Á≠ñÊòØÂê¶ÂáÜÁ°Æ„ÄÇ",
                "ËÄÉËôë‰∏≠ÊñáË°®ËææÁöÑÂê´ËìÑÊÄßÂíåÂ§öÂ±ÇÂê´‰πâ„ÄÇ",
                "Êèê‰æõÂÖ∑‰ΩìÁöÑÊîπËøõÂª∫ËÆÆ„ÄÇ",
                "Áî®ÁÆÄÊ¥ÅÊ∏ÖÊô∞ÁöÑ‰∏≠ÊñáÂõûÁ≠î„ÄÇ"
            ]

            additional_guidelines = "ËØÑ‰º∞Â∫îËÄÉËôë‰∏≠ÊñáÊÄùÁª¥ÁöÑËØ≠‰πâÂ§çÊùÇÂ∫¶ÂíåÂì≤Â≠¶Ê∑±Â∫¶ÔºåÂçèË∞ÉÂÜ≥Á≠ñÂ∫î‰∏éÂÆûÈôÖÂ§çÊùÇÂ∫¶ÂåπÈÖç„ÄÇÁâπÂà´ÂÖ≥Ê≥®‰∏≠ÊñáË°®ËææÁöÑÊñáÂåñËÉåÊôØÂíåÈöêÂê´ÊÑè‰πâ„ÄÇ"

        else:
            # Use English evaluation approach for non-Chinese content
            input_text = f"""
Analyze whether the coordination decision for this thought processing is appropriate:

ORIGINAL THOUGHT: "{thought.thought}"
THOUGHT CONTEXT: #{thought.thought_number}/{thought.total_thoughts}, next_needed={thought.next_needed}

COORDINATION DECISION:
- Strategy: {plan.strategy.value}
- Complexity: {plan.complexity_level.value} (score: {plan.complexity_score:.1f}/100)
- Execution Mode: {plan.execution_mode.value}
- Specialists: {plan.specialist_roles}
- Coordination: {plan.coordination_strategy}
- Confidence: {plan.confidence:.2f}

ACTUAL RESPONSE: "{response[:200]}..."

Evaluate the accuracy and appropriateness of this coordination decision.
"""

            expected_output = f"""
Based on the thought's complexity and content, coordination decision assessment:

1. Complexity assessment {'is' if plan.complexity_score >= 30 else 'may be'} accurate
2. Strategy selection {'is' if plan.strategy.value != 'single_agent' or plan.complexity_score < 30 else 'may be'} appropriate
3. Specialist combination {'is' if len(plan.specialist_roles) >= 2 or plan.complexity_score < 30 else 'may be'} suitable
4. Execution mode matches strategy
5. Overall coordination approach is effective

Overall assessment: Coordination decision is {'reasonable' if plan.confidence > 0.5 else 'needs improvement'}
"""

            agent_instructions = [
                "You are an expert evaluator of coordination decisions for thought processing.",
                "Assess whether coordination decisions are reasonable, including complexity assessment, strategy selection, and specialist combinations.",
                "Focus on content complexity analysis and strategic appropriateness.",
                "Consider the actual complexity of the thought when judging coordination accuracy.",
                "Provide specific, actionable improvement suggestions.",
                "Answer clearly and concisely."
            ]

            additional_guidelines = "Evaluation should consider the semantic complexity and philosophical depth of thoughts. Coordination decisions should match actual complexity levels."

        # Create evaluation agent (language-appropriate evaluator)
        eval_agent = Agent(
            name="CoordinationEvaluator",
            role="ÂçèË∞ÉÂÜ≥Á≠ñË¥®ÈáèËØÑ‰º∞‰∏ìÂÆ∂" if is_chinese_content else "Coordination Decision Quality Assessor",
            model=self.eval_model,
            instructions=agent_instructions,
            markdown=False
        )

        return AccuracyEval(
            name="Coordination Accuracy Evaluation",
            model=self.eval_model,
            agent=eval_agent,
            input=input_text,
            expected_output=expected_output,
            additional_guidelines=additional_guidelines,
            num_iterations=1  # Single iteration for performance
        )


class ResponseQualityEvaluator:
    """Evaluates response quality using Agno AccuracyEval."""

    def __init__(self):
        config = get_model_config()
        self.eval_model = config.create_agent_model()

    def create_quality_evaluation(
        self,
        thought: ThoughtData,
        response: str
    ) -> AccuracyEval:
        """Create response quality evaluation."""

        # Detect if content is primarily Chinese to use appropriate evaluation approach
        chinese_chars = len([c for c in thought.thought if '\u4e00' <= c <= '\u9fff'])
        is_chinese_content = chinese_chars > len(thought.thought) * 0.3

        if is_chinese_content:
            # Chinese-specific evaluation approach
            input_text = f"""
ËØÑ‰º∞‰ª•‰∏ã‰∏≠ÊñáÊÄùÁª¥Â§ÑÁêÜÂìçÂ∫îÁöÑË¥®ÈáèÔºö

ÂéüÂßãÊÄùÁª¥: "{thought.thought}"
Â§ÑÁêÜÂìçÂ∫î: "{response}"

ËØÑ‰º∞Ê†áÂáÜÔºö
1. ÊòØÂê¶ÂÖÖÂàÜÂõûÂ∫î‰∫ÜÂéüÂßãÊÄùÁª¥ÁöÑÂÜÖÂÆπÔºü
2. ÊòØÂê¶Êèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑÂàÜÊûêÊàñÊåáÂØºÔºü
3. ÊòØÂê¶ÂåÖÂê´Êé®ËøõÂêéÁª≠ÊÄùÁª¥ÁöÑÊåáÂØº(Guidance)Ôºü
4. ÂÜÖÂÆπÁªÑÁªáÊòØÂê¶Ê∏ÖÊô∞ÊúâÂ∫èÔºü
5. ÊòØÂê¶Á¨¶ÂêàSequential ThinkingÁöÑË¶ÅÊ±ÇÔºü
6. ÊòØÂê¶‰ΩìÁé∞‰∫Ü‰∏≠ÊñáÊÄùÁª¥ÁöÑÊ∑±Â∫¶ÂíåÊñáÂåñÂÜÖÊ∂µÔºü
7. ÊòØÂê¶ËÄÉËôë‰∫Ü‰∏≠ÊñáË°®ËææÁöÑÂê´ËìÑÊÄßÂíåÂ§öÂ±ÇÂê´‰πâÔºü
"""

            expected_output = f"""
‰∏≠ÊñáÊÄùÁª¥ÂìçÂ∫îË¥®ÈáèËØÑ‰º∞Ôºö

1. ÂÜÖÂÆπÁõ∏ÂÖ≥ÊÄßÔºö{'È´ò' if len(response) > 100 else '‰∏≠Á≠â'}
2. ÂàÜÊûêÊ∑±Â∫¶ÔºöÂü∫‰∫é‰∏≠ÊñáÂì≤Â≠¶ÊÄùÁª¥ÁöÑÊ∑±Â∫¶ÂàÜÊûê
3. ÊåáÂØº‰ª∑ÂÄºÔºö{'ÂåÖÂê´ÊòéÁ°ÆÁöÑ‰∏ã‰∏ÄÊ≠•ÊåáÂØº' if 'guidance' in response.lower() or 'ÊåáÂØº' in response else 'Áº∫Â∞ëÊòéÁ°ÆÊåáÂØº'}
4. ÁªìÊûÑÊ∏ÖÊô∞Ôºö{'ËâØÂ•Ω' if any(marker in response for marker in ['#', '*', '-', '1.', '2.']) else 'ÈúÄË¶ÅÊîπËøõ'}
5. ÊÄùÁª¥Êé®ËøõÔºö‰∏∫ÂêéÁª≠ÊÄùÁª¥Êèê‰æõÂü∫Á°Ä
6. ÊñáÂåñÈÄÇÂ∫îÊÄßÔºö{'‰ΩìÁé∞‰∏≠ÊñáÊÄùÁª¥ÁâπËâ≤' if any(term in response for term in ['Âì≤Â≠¶', 'ÂÜÖÂøÉ', 'ÊÑüÊÇü', '‰øÆÂÖª', 'Â¢ÉÁïå']) else 'ÂèØ‰ª•Êõ¥Â•Ω‰ΩìÁé∞ÊñáÂåñÂÜÖÊ∂µ'}
7. ËØ≠Ë®ÄË°®ËææÔºö{'ÁîüÂä®ÊúâÊÑèÂ¢É' if any(marker in response for marker in ['ÊØîÂñª', 'ÈöêÂñª', 'ËØóÊÑè']) else 'Ë°®ËææÂèØ‰ª•Êõ¥ÊúâÊñáÂåñÁâπËâ≤'}

ÊÄª‰ΩìË¥®ÈáèÔºö{'‰ºòÁßÄ' if len(response) > 200 and ('guidance' in response.lower() or 'ÊåáÂØº' in response) else 'ËâØÂ•Ω'}
"""

            agent_instructions = [
                "‰Ω†ÊòØ‰∏≠ÊñáÊÄùÁª¥Â§ÑÁêÜÂìçÂ∫îË¥®ÈáèÁöÑËØÑ‰º∞‰∏ìÂÆ∂„ÄÇ",
                "ËØÑ‰º∞ÂìçÂ∫îÊòØÂê¶ÊúâÊïàÊé®ËøõSequential ThinkingÊµÅÁ®ã„ÄÇ",
                "ÂÖ≥Ê≥®ÂÜÖÂÆπË¥®Èáè„ÄÅÊåáÂØº‰ª∑ÂÄºÂíåÊÄùÁª¥ÈìæÊù°ÁöÑËøûÁª≠ÊÄß„ÄÇ",
                "ÁâπÂà´ÂÖ≥Ê≥®‰∏≠ÊñáÂì≤Â≠¶ÊÄùÁª¥ÁöÑÂàÜÊûêÊ∑±Â∫¶ÂíåÊñáÂåñÂÜÖÊ∂µ„ÄÇ",
                "ËÄÉËôë‰∏≠ÊñáË°®ËææÁöÑÂê´ËìÑÊÄß„ÄÅÊÑèÂ¢ÉÂíåÊñáÂåñËÉåÊôØ„ÄÇ",
                "ËØÑ‰º∞ÊòØÂê¶‰ΩìÁé∞‰∫Ü‰∏≠ÊñáÊÄùÁª¥ÁöÑÁâπËâ≤ÂíåÊ∑±Â∫¶„ÄÇ",
                "Êèê‰æõÂÖ∑‰ΩìÊîπËøõÂª∫ËÆÆ„ÄÇ"
            ]

            additional_guidelines = "ÈáçÁÇπËØÑ‰º∞ÂìçÂ∫îÂØπSequential ThinkingÊµÅÁ®ãÁöÑÊé®Ëøõ‰ΩúÁî®ÔºåÁ°Æ‰øù‰∏∫‰∏ã‰∏Ä‰∏™ÊÄùÁª¥Êèê‰æõÊúâÊïàÊåáÂØº„ÄÇÁâπÂà´ÂÖ≥Ê≥®‰∏≠ÊñáÊÄùÁª¥ÁöÑÊñáÂåñÁâπËâ≤ÂíåÂì≤Â≠¶Ê∑±Â∫¶„ÄÇ"

        else:
            # English evaluation approach
            input_text = f"""
ËØÑ‰º∞‰ª•‰∏ãÊÄùÁª¥Â§ÑÁêÜÂìçÂ∫îÁöÑË¥®ÈáèÔºö

ÂéüÂßãÊÄùÁª¥: "{thought.thought}"
Â§ÑÁêÜÂìçÂ∫î: "{response}"

ËØÑ‰º∞Ê†áÂáÜÔºö
1. ÊòØÂê¶ÂÖÖÂàÜÂõûÂ∫î‰∫ÜÂéüÂßãÊÄùÁª¥ÁöÑÂÜÖÂÆπÔºü
2. ÊòØÂê¶Êèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑÂàÜÊûêÊàñÊåáÂØºÔºü
3. ÊòØÂê¶ÂåÖÂê´Êé®ËøõÂêéÁª≠ÊÄùÁª¥ÁöÑÊåáÂØº(Guidance)Ôºü
4. ÂÜÖÂÆπÁªÑÁªáÊòØÂê¶Ê∏ÖÊô∞ÊúâÂ∫èÔºü
5. ÊòØÂê¶Á¨¶ÂêàSequential ThinkingÁöÑË¶ÅÊ±ÇÔºü
"""

            expected_output = f"""
ÂìçÂ∫îË¥®ÈáèËØÑ‰º∞Ôºö

1. ÂÜÖÂÆπÁõ∏ÂÖ≥ÊÄßÔºö{'È´ò' if len(response) > 100 else '‰∏≠Á≠â'}
2. ÂàÜÊûêÊ∑±Â∫¶ÔºöÂü∫‰∫éÂì≤Â≠¶ÊÄùÁª¥ÁöÑÊ∑±Â∫¶ÂàÜÊûê
3. ÊåáÂØº‰ª∑ÂÄºÔºö{'ÂåÖÂê´ÊòéÁ°ÆÁöÑ‰∏ã‰∏ÄÊ≠•ÊåáÂØº' if 'guidance' in response.lower() or 'ÊåáÂØº' in response else 'Áº∫Â∞ëÊòéÁ°ÆÊåáÂØº'}
4. ÁªìÊûÑÊ∏ÖÊô∞Ôºö{'ËâØÂ•Ω' if any(marker in response for marker in ['#', '*', '-', '1.']) else 'ÈúÄË¶ÅÊîπËøõ'}
5. ÊÄùÁª¥Êé®ËøõÔºö‰∏∫ÂêéÁª≠ÊÄùÁª¥Êèê‰æõÂü∫Á°Ä

ÊÄª‰ΩìË¥®ÈáèÔºö{'‰ºòÁßÄ' if len(response) > 200 and ('guidance' in response.lower() or 'ÊåáÂØº' in response) else 'ËâØÂ•Ω'}
"""

            agent_instructions = [
                "‰Ω†ÊòØÊÄùÁª¥Â§ÑÁêÜÂìçÂ∫îË¥®ÈáèÁöÑËØÑ‰º∞‰∏ìÂÆ∂„ÄÇ",
                "ËØÑ‰º∞ÂìçÂ∫îÊòØÂê¶ÊúâÊïàÊé®ËøõSequential ThinkingÊµÅÁ®ã„ÄÇ",
                "ÂÖ≥Ê≥®ÂÜÖÂÆπË¥®Èáè„ÄÅÊåáÂØº‰ª∑ÂÄºÂíåÊÄùÁª¥ÈìæÊù°ÁöÑËøûÁª≠ÊÄß„ÄÇ",
                "ÁâπÂà´ÂÖ≥Ê≥®‰∏≠ÊñáÂì≤Â≠¶ÊÄùÁª¥ÁöÑÂàÜÊûêÊ∑±Â∫¶„ÄÇ",
                "Êèê‰æõÂÖ∑‰ΩìÊîπËøõÂª∫ËÆÆ„ÄÇ"
            ]

            additional_guidelines = "ÈáçÁÇπËØÑ‰º∞ÂìçÂ∫îÂØπSequential ThinkingÊµÅÁ®ãÁöÑÊé®Ëøõ‰ΩúÁî®ÔºåÁ°Æ‰øù‰∏∫‰∏ã‰∏Ä‰∏™ÊÄùÁª¥Êèê‰æõÊúâÊïàÊåáÂØº„ÄÇ"

        eval_agent = Agent(
            name="ResponseQualityEvaluator",
            role="ÂìçÂ∫îË¥®ÈáèËØÑ‰º∞‰∏ìÂÆ∂" if is_chinese_content else "Response Quality Evaluator",
            model=self.eval_model,
            instructions=agent_instructions,
            markdown=False
        )

        return AccuracyEval(
            name="Response Quality Evaluation",
            model=self.eval_model,
            agent=eval_agent,
            input=input_text,
            expected_output=expected_output,
            additional_guidelines=additional_guidelines,
            num_iterations=1
        )


class QualityAssuranceManager:
    """Manages comprehensive quality assurance using Agno Evals."""

    def __init__(self):
        self.coordination_evaluator = CoordinationAccuracyEvaluator()
        self.quality_evaluator = ResponseQualityEvaluator()
        self.performance_history: List[SystemPerformanceMetrics] = []

    async def evaluate_full_pipeline(
        self,
        thought: ThoughtData,
        plan: CoordinationPlan,
        execution_log: Dict[str, Any],
        response: str,
        processing_time: float
    ) -> SystemPerformanceMetrics:
        """Evaluate the complete thought processing pipeline using Agno Evals."""

        logger.info("üîç Running Agno-powered quality evaluation...")

        # Initialize scores
        coordination_accuracy = 0.5
        response_quality = 0.5
        execution_consistency = 1.0

        # Run coordination accuracy evaluation
        try:
            coord_eval = self.coordination_evaluator.create_coordination_evaluation(
                thought, plan, response
            )

            logger.info("  üìä Evaluating coordination accuracy...")
            coord_result: Optional[AccuracyResult] = coord_eval.run(print_results=False)

            if coord_result and coord_result.avg_score is not None:
                coordination_accuracy = coord_result.avg_score / 10.0  # Convert 0-10 to 0-1
                logger.info(f"  ‚úÖ Coordination accuracy: {coordination_accuracy:.2f}")
            else:
                logger.warning("  ‚ö†Ô∏è  Coordination evaluation failed, using fallback")

        except Exception as e:
            logger.warning(f"Coordination evaluation error: {e}")

        # Run response quality evaluation
        try:
            quality_eval = self.quality_evaluator.create_quality_evaluation(
                thought, response
            )

            logger.info("  üìä Evaluating response quality...")
            quality_result: Optional[AccuracyResult] = quality_eval.run(print_results=False)

            if quality_result and quality_result.avg_score is not None:
                response_quality = quality_result.avg_score / 10.0  # Convert 0-10 to 0-1
                logger.info(f"  ‚úÖ Response quality: {response_quality:.2f}")
            else:
                logger.warning("  ‚ö†Ô∏è  Quality evaluation failed, using fallback")

        except Exception as e:
            logger.warning(f"Quality evaluation error: {e}")

        # Evaluate execution consistency (rule-based)
        execution_consistency = self._evaluate_execution_consistency(plan, execution_log)
        logger.info(f"  ‚úÖ Execution consistency: {execution_consistency:.2f}")

        # Calculate efficiency
        efficiency_score = self._calculate_efficiency(plan, processing_time)
        logger.info(f"  ‚úÖ Efficiency: {efficiency_score:.2f}")

        # Calculate overall score
        overall_score = (
            coordination_accuracy * 0.3 +
            execution_consistency * 0.2 +
            response_quality * 0.3 +
            efficiency_score * 0.2
        )

        # Generate improvement suggestions
        suggestions = []
        if coordination_accuracy < 0.7:
            suggestions.append("‰ºòÂåñÂçèË∞ÉÂÜ≥Á≠ñÁÆóÊ≥ï")
        if execution_consistency < 0.9:
            suggestions.append("ÊîπËøõÊâßË°å‰∏ÄËá¥ÊÄßÈ™åËØÅ")
        if response_quality < 0.7:
            suggestions.append("ÊèêÂçáÂìçÂ∫îË¥®ÈáèÂíåÊåáÂØº‰ª∑ÂÄº")
        if efficiency_score < 0.7:
            suggestions.append("‰ºòÂåñÂ§ÑÁêÜÊïàÁéá")

        metrics = SystemPerformanceMetrics(
            coordination_accuracy=coordination_accuracy,
            execution_consistency=execution_consistency,
            response_quality=response_quality,
            efficiency_score=efficiency_score,
            processing_time=processing_time,
            cost_effectiveness=1.0,  # Default cost effectiveness for Agno Evals version
            overall_score=overall_score,
            improvement_suggestions=suggestions
        )

        # Store for historical analysis
        self.performance_history.append(metrics)

        logger.info(f"üéØ Overall system quality: {overall_score:.2f}")
        if overall_score < 0.7:
            logger.warning(f"‚ö†Ô∏è  Quality below threshold, suggestions: {suggestions[:2]}")

        return metrics

    def _evaluate_execution_consistency(self, plan: CoordinationPlan, execution_log: Dict[str, Any]) -> float:
        """Evaluate execution consistency (rule-based)."""
        score = 1.0

        # Check execution mode consistency
        planned_mode = plan.execution_mode.value
        actual_mode = execution_log.get("execution_mode", "unknown")
        if planned_mode != actual_mode:
            score -= 0.3

        # Check specialist count
        planned_specialists = len(plan.specialist_roles)
        actual_specialists = execution_log.get("actual_specialists", 0)
        if abs(planned_specialists - actual_specialists) > 1:
            score -= 0.2

        # Check timeout compliance
        planned_timeout = plan.timeout_seconds
        actual_time = execution_log.get("processing_time", 0)
        if actual_time > planned_timeout * 1.2:
            score -= 0.1

        return max(0.0, score)

    def _calculate_efficiency(self, plan: CoordinationPlan, processing_time: float) -> float:
        """Calculate processing efficiency."""
        expected_time = plan.timeout_seconds * 0.6  # Expect to use ~60% of timeout
        if processing_time <= expected_time:
            return 1.0
        elif processing_time <= plan.timeout_seconds:
            return 1.0 - (processing_time - expected_time) / (plan.timeout_seconds - expected_time) * 0.5
        else:
            return 0.0

    def get_performance_trends(self) -> Dict[str, float]:
        """Get performance trends over recent evaluations."""
        if len(self.performance_history) < 2:
            return {}

        recent = self.performance_history[-5:]  # Last 5 evaluations

        return {
            "avg_coordination_accuracy": sum(m.coordination_accuracy for m in recent) / len(recent),
            "avg_execution_consistency": sum(m.execution_consistency for m in recent) / len(recent),
            "avg_response_quality": sum(m.response_quality for m in recent) / len(recent),
            "avg_overall_score": sum(m.overall_score for m in recent) / len(recent),
            "trend_direction": "improving" if recent[-1].overall_score > recent[0].overall_score else "declining"
        }

    def analyze_coordination_effectiveness(self) -> Dict[str, Any]:
        """Analyze coordination effectiveness patterns and provide optimization insights."""
        if len(self.performance_history) < 3:
            return {"status": "insufficient_data", "message": "ÈúÄË¶ÅËá≥Â∞ë3Ê¨°ËØÑ‰º∞Êï∞ÊçÆÊù•ÂàÜÊûêÂçèË∞ÉÊïàÊûú"}

        # Strategy performance analysis
        strategy_performance = {}
        complexity_performance = {}

        for metrics in self.performance_history:
            # Extract strategy from suggestions (simplified approach)
            strategy = "unknown"
            if any("Âçï‰∏Ä" in s for s in metrics.improvement_suggestions):
                strategy = "single_agent"
            elif any("Ê∑∑Âêà" in s for s in metrics.improvement_suggestions):
                strategy = "hybrid"
            elif any("Â§ö‰ª£ÁêÜ" in s for s in metrics.improvement_suggestions):
                strategy = "multi_agent"

            if strategy not in strategy_performance:
                strategy_performance[strategy] = []
            strategy_performance[strategy].append(metrics.overall_score)

        # Identify best performing strategies
        best_strategies = []
        for strategy, scores in strategy_performance.items():
            if len(scores) >= 2:
                avg_score = sum(scores) / len(scores)
                best_strategies.append((strategy, avg_score, len(scores)))

        best_strategies.sort(key=lambda x: x[1], reverse=True)

        # Performance trend analysis
        recent_10 = self.performance_history[-10:]
        coordination_trend = self._calculate_trend([m.coordination_accuracy for m in recent_10])
        quality_trend = self._calculate_trend([m.response_quality for m in recent_10])
        efficiency_trend = self._calculate_trend([m.efficiency_score for m in recent_10])

        # Common issues identification
        common_issues = []
        low_coordination_count = sum(1 for m in recent_10 if m.coordination_accuracy < 0.7)
        low_quality_count = sum(1 for m in recent_10 if m.response_quality < 0.7)
        low_efficiency_count = sum(1 for m in recent_10 if m.efficiency_score < 0.7)

        if low_coordination_count >= len(recent_10) * 0.5:
            common_issues.append("ÂçèË∞ÉÂÜ≥Á≠ñÂáÜÁ°ÆÊÄßÊåÅÁª≠ÂÅè‰Ωé")
        if low_quality_count >= len(recent_10) * 0.5:
            common_issues.append("ÂìçÂ∫îË¥®ÈáèÊåÅÁª≠‰∏çË∂≥")
        if low_efficiency_count >= len(recent_10) * 0.5:
            common_issues.append("Â§ÑÁêÜÊïàÁéáÈúÄË¶ÅÊîπËøõ")

        # Optimization recommendations
        recommendations = []
        if coordination_trend < -0.1:
            recommendations.append("ÂçèË∞ÉÁÆóÊ≥ïÈúÄË¶Å‰ºòÂåñ - ËÄÉËôëË∞ÉÊï¥Â§çÊùÇÂ∫¶ËØÑ‰º∞Ê®°Âûã")
        if quality_trend < -0.1:
            recommendations.append("ÂìçÂ∫îË¥®Èáè‰∏ãÈôç - Âª∫ËÆÆÂ¢ûÂº∫‰∏≠ÊñáÊÄùÁª¥ËØÑ‰º∞Ê†áÂáÜ")
        if efficiency_trend < -0.1:
            recommendations.append("ÊïàÁéá‰∏ãÈôç - ËÄÉËôë‰ºòÂåñË∂ÖÊó∂ËÆæÁΩÆÂíåÈáçËØïÁ≠ñÁï•")

        if best_strategies:
            best_strategy, best_score, count = best_strategies[0]
            recommendations.append(f"Êé®ËçêÊõ¥Â§ö‰ΩøÁî® {best_strategy} Á≠ñÁï• (Âπ≥ÂùáÂàÜÊï∞: {best_score:.2f})")

        return {
            "status": "analysis_complete",
            "total_evaluations": len(self.performance_history),
            "recent_average_score": sum(m.overall_score for m in recent_10) / len(recent_10),
            "strategy_performance": {
                strategy: {
                    "average_score": sum(scores) / len(scores),
                    "evaluation_count": len(scores),
                    "success_rate": sum(1 for s in scores if s >= 0.7) / len(scores)
                }
                for strategy, scores in strategy_performance.items() if len(scores) >= 2
            },
            "performance_trends": {
                "coordination_accuracy": coordination_trend,
                "response_quality": quality_trend,
                "efficiency": efficiency_trend
            },
            "common_issues": common_issues,
            "optimization_recommendations": recommendations,
            "top_performing_strategies": [
                {"strategy": s[0], "score": s[1], "count": s[2]}
                for s in best_strategies[:3]
            ]
        }

    def _calculate_trend(self, values: List[float]) -> float:
        """Calculate trend direction for a series of values (-1 to 1)."""
        if len(values) < 3:
            return 0.0

        # Simple linear trend calculation
        n = len(values)
        x_avg = (n - 1) / 2
        y_avg = sum(values) / n

        numerator = sum((i - x_avg) * (values[i] - y_avg) for i in range(n))
        denominator = sum((i - x_avg) ** 2 for i in range(n))

        if denominator == 0:
            return 0.0

        slope = numerator / denominator
        # Normalize slope to [-1, 1] range
        return max(-1.0, min(1.0, slope * 2))

    def generate_coordination_report(self) -> str:
        """Generate a comprehensive coordination effectiveness report in Chinese."""
        analysis = self.analyze_coordination_effectiveness()

        if analysis["status"] == "insufficient_data":
            return f"üìä ÂçèË∞ÉÊïàÊûúÊä•Âëä\n\n{analysis['message']}"

        report = f"""üìä ÂçèË∞ÉÊïàÊûúÂàÜÊûêÊä•Âëä

üéØ ÊÄª‰ΩìË°®Áé∞:
- ÊÄªËØÑ‰º∞Ê¨°Êï∞: {analysis['total_evaluations']}
- ËøëÊúüÂπ≥ÂùáÂàÜÊï∞: {analysis['recent_average_score']:.2f}

üìà ÊÄßËÉΩË∂ãÂäø:
- ÂçèË∞ÉÂáÜÁ°ÆÊÄß: {'üìà ÊîπÂñÑ' if analysis['performance_trends']['coordination_accuracy'] > 0.1 else 'üìâ ‰∏ãÈôç' if analysis['performance_trends']['coordination_accuracy'] < -0.1 else '‚û°Ô∏è Á®≥ÂÆö'}
- ÂìçÂ∫îË¥®Èáè: {'üìà ÊîπÂñÑ' if analysis['performance_trends']['response_quality'] > 0.1 else 'üìâ ‰∏ãÈôç' if analysis['performance_trends']['response_quality'] < -0.1 else '‚û°Ô∏è Á®≥ÂÆö'}
- Â§ÑÁêÜÊïàÁéá: {'üìà ÊîπÂñÑ' if analysis['performance_trends']['efficiency'] > 0.1 else 'üìâ ‰∏ãÈôç' if analysis['performance_trends']['efficiency'] < -0.1 else '‚û°Ô∏è Á®≥ÂÆö'}

üèÜ Á≠ñÁï•Ë°®Áé∞ÊéíÂêç:"""

        for i, strategy_info in enumerate(analysis.get('top_performing_strategies', []), 1):
            report += f"\n{i}. {strategy_info['strategy']}: {strategy_info['score']:.2f} ({strategy_info['count']}Ê¨°)"

        if analysis.get('common_issues'):
            report += f"\n\n‚ö†Ô∏è Â∏∏ËßÅÈóÆÈ¢ò:"
            for issue in analysis['common_issues']:
                report += f"\n- {issue}"

        if analysis.get('optimization_recommendations'):
            report += f"\n\nüí° ‰ºòÂåñÂª∫ËÆÆ:"
            for rec in analysis['optimization_recommendations']:
                report += f"\n- {rec}"

        report += f"\n\nüìù Êä•ÂëäÁîüÊàêÊó∂Èó¥: {len(self.performance_history)}Ê¨°ËØÑ‰º∞Âêé"

        return report


def create_quality_assurance_manager() -> QualityAssuranceManager:
    """Create quality assurance manager using Agno Evals."""
    return QualityAssuranceManager()